{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for Earth System Science Hackathon 2020\n",
    "# Challenge Notebook Template\n",
    "Author (Instituion), Second Author (Institution)\n",
    "\n",
    "## Introduction\n",
    "*A relevant picture*\n",
    "\n",
    "The introduction contains the following elements:\n",
    "* Scientific goal of the challenge\n",
    "* Contextual background on the problem\n",
    "* Short description of existing solutions (if any)\n",
    "* Why the problem is important\n",
    "* Impact if solved\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Requirements\n",
    "This notebook requires Python >= 3.7. The following libraries are required:\n",
    "* numpy\n",
    "* scipy\n",
    "* matplotlib\n",
    "* xarray\n",
    "* pandas\n",
    "* scikit-learn\n",
    "* tensorflow >= 2.1\n",
    "* netcdf4\n",
    "* tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (1.17.4)\n",
      "Requirement already satisfied: scipy in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (1.3.2)\n",
      "Requirement already satisfied: matplotlib in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (3.1.1)\n",
      "Requirement already satisfied: xarray in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (0.15.1)\n",
      "Requirement already satisfied: pandas in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (0.25.3)\n",
      "Requirement already satisfied: netcdf4 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (1.5.3)\n",
      "Requirement already satisfied: tqdm in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (4.40.2)\n",
      "Requirement already satisfied: tensorflow in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (2.0.0)\n",
      "Requirement already satisfied: scikit-learn in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (0.22)\n",
      "Requirement already satisfied: dask in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (2.9.0)\n",
      "Requirement already satisfied: s3fs in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (0.4.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from matplotlib) (2.4.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: setuptools>=41.2 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from xarray) (42.0.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: cftime in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from netcdf4) (1.0.4.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorflow) (1.25.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorflow) (0.33.6)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorflow) (3.11.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorflow) (0.1.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages/wrapt-1.11.2-py3.7-linux-x86_64.egg (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorflow) (1.13.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from scikit-learn) (0.14.1)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from s3fs) (1.13.36)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from s3fs) (0.7.4)\n",
      "Requirement already satisfied: h5py in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from botocore>=1.12.91->s3fs) (0.15.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from botocore>=1.12.91->s3fs) (0.9.4)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version >= \"3.4\" in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from botocore>=1.12.91->s3fs) (1.25.7)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.2.7)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /glade/work/cbecker/ncar_20191211/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/glade/work/cbecker/ncar_20191211/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy scipy matplotlib xarray pandas netcdf4 tqdm tensorflow scikit-learn dask s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.distributed import Client, LocalCluster, progress\n",
    "import s3fs\n",
    "from glob import glob\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Set random seed\n",
    "seed = 3985\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "The data summary should contain the following pieces of information:\n",
    "* Data generation procedure (satellite, model, etc.) \n",
    "* Link to website containing more information about dataset\n",
    "* Time span of the dataset\n",
    "* Geographic coverage of the dataset\n",
    "* Parameter space coverage (if synthetic)\n",
    "\n",
    "### Potential Input Variables\n",
    "| Variable Name | Units | Description | Relevance |\n",
    "| ------------- | :----:|:----------- | :--------:|\n",
    "| ABI Band 08   | K     | Upper-level Water Vapor | ¯\\\\_(ツ)_/¯ |\n",
    "| ABI Band 09   | K     | Mid-level Water Vapor   |\n",
    "| ABI Band 10   | K     | Lower-level Water Vapor |\n",
    "| ABI Band 14   | K     | Longwave Window         |\n",
    "\n",
    "### Output Variables\n",
    "| Variable Name | Units | Description |\n",
    "| ------------- | :----:|:----------- |\n",
    "| GLM Counts    | -     | Lightning strike count |\n",
    "\n",
    "\n",
    "### Metadata Variables\n",
    "| Variable Name | Units | Description |\n",
    "| ------------- | :----:|:----------- |\n",
    "| Time     | YYYY-MM-DDTHH:MM:SS  | The Date   |\n",
    "| Lat      | degrees     | Latitude   |\n",
    "| Lon      | degrees     | Longitude  |\n",
    "\n",
    "\n",
    "### Training Set\n",
    "Description of training set time/space/parameter coverage and size\n",
    "\n",
    "### Validation Set\n",
    "Description of validation set time/space/parameter coverage and size\n",
    "\n",
    "### Test Set\n",
    "Description of test set time/space/parameter coverage and size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to load the data from disk or cloud\n",
    "\n",
    "def split_data_files(dir_path=\"ncar-aiml-data-commons/goes/ABI_patches_32/\", file_prefix='abi_patches_', \n",
    "               start_date='20190306', end_date='20190901', seq_len=4, skip_len=1):\n",
    "    \"\"\"\n",
    "    Take daily ABI patch files and split into equal training/validation/testing\n",
    "    semi-contiguous partitions, skipping day(s) between chunks to isolate convective \n",
    "    cycles.\n",
    "    \n",
    "    Args: \n",
    "        dir_path: (str) Directory path to daily ABI files\n",
    "        file_prefix: (str) File prefix up to date \n",
    "        start_date: (str) Starting date to get files in format of YYYYMMDD\n",
    "        end_date: (str) Ending date to get files in format of YYYYMMDD\n",
    "        seq_len: (int) Length of days per 'chunk' of data\n",
    "        skip_len: (int) How many days to skip between data chunks\n",
    "        \n",
    "    Returns:\n",
    "        train_f, val_f, test_f: list of training/validation/test files\n",
    "    \"\"\"\n",
    "    \n",
    "    all_files = fs.ls(dir_path)\n",
    "    start_index = all_files.index('{}{}{}T000000.nc'.format(dir_path, file_prefix, start_date))\n",
    "    end_index = all_files.index('{}{}{}T000000.nc'.format(dir_path, file_prefix, end_date))\n",
    "    file_spread = all_files[start_index:end_index+1]\n",
    "    \n",
    "    train_files, val_files, test_files = [], [], []\n",
    "    \n",
    "    for i in np.arange(0, len(file_spread)+1, (seq_len+skip_len)*3):\n",
    "        \n",
    "        val_i = i + seq_len + skip_len\n",
    "        test_i = i + (seq_len + skip_len)*2\n",
    "        \n",
    "        train_files.append(file_spread[i:i+seq_len])\n",
    "        val_files.append(file_spread[val_i:val_i+seq_len])\n",
    "        test_files.append(file_spread[test_i:test_i+seq_len])\n",
    "        \n",
    "    train_f = [item for sublist in train_files for item in sublist]\n",
    "    val_f = [item for sublist in val_files for item in sublist]\n",
    "    test_f = [item for sublist in test_files for item in sublist]\n",
    "    \n",
    "    return train_f, val_f, test_f\n",
    "\n",
    "def fetch_data(file_number, file_list):\n",
    "    \"\"\"\n",
    "    Function to be distributed across a cluster to individually load files directly from an AWS S3 bucket \n",
    "    \n",
    "    Args:\n",
    "        file_number: index for file from file_list\n",
    "        file_list: List of files to index from\n",
    "    Returns:\n",
    "        ds: xarray dataset of daily file \n",
    "    \"\"\"\n",
    "    obj = fs.open(file_list[file_number])\n",
    "    ds = xr.open_dataset(obj, chunks={})\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def merge_data(file_list):\n",
    "    \"\"\"\n",
    "    Take a list of files and distribute across a cluster to be loaded then gathered and concantenated\n",
    "    \n",
    "    Args:\n",
    "        file_list: List of files to be merged together (training, validation, or testing)\n",
    "    Returns:\n",
    "        merged_data: Concatenated xarray dataset of training, validation, or testing data \n",
    "    \"\"\"\n",
    "    futures = client.map(fetch_data, range(len(file_list)), [file_list]*len(file_list))\n",
    "    results = client.gather(futures)\n",
    "    merged_data = xr.concat(results, 'patch').compute()\n",
    "    \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(processes=True, threads_per_worker=2)\n",
    "client = Client(cluster)\n",
    "fs = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.1 s, sys: 21.4 s, total: 41.4 s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_files, val_files, test_files = split_data_files()\n",
    "train, val, test = map(merge_data, [train_files, val_files, test_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input, output and meta data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training, validation, and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory visualizations of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transforms\n",
    "Discuss any transforms or normalizations that may be needed for this dataset. Remember to fit a scaler only to the training data and then apply it on testing and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of data transform procedure for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual of input variable before and after transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Machine Learning Model\n",
    "Description of baseline ML approach should include:\n",
    "* Choice of ML software\n",
    "* Type of ML model\n",
    "* Hyperparameter choices and justification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline ML model initialization code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "Description of the different metrics used to assess performance on the challenge:\n",
    "* Correctness Metric: how close are the predictions to the truth (e.g., RMSE or AUC) \n",
    "* Training time\n",
    "* Inference time\n",
    "* Model complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "Description of interpretation methods for problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include examples of interpretation code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hackathon Challenges\n",
    "\n",
    "### Monday\n",
    "* Load the data\n",
    "* Create an exploratory visualization of the data\n",
    "* Test two different transformation and scaling methods\n",
    "* Test one dimensionality reduction method\n",
    "* Train a linear model\n",
    "* Train a decision tree ensemble method of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monday's code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuesday\n",
    "* Train a densely connected neural network\n",
    "* Train a convolutional or recurrent neural network (depends on problem)\n",
    "* Experiment with different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuesday's code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wednesday\n",
    "* Calculate three relevant evaluation metrics for each ML solution and baseline\n",
    "* Refine machine learning approaches and test additional hyperparameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wednesday's code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thursday \n",
    "* Evaluate two interpretation methods for your machine learning solution\n",
    "* Compare interpretation of baseline with your approach\n",
    "* Submit best results on project to leaderboard\n",
    "* Prepare 2 Google Slides on team's approach and submit them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday's code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ultimate Submission Code\n",
    "Please insert your full data processing and machine learning pipeline code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
