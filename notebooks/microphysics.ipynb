{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for Earth System Science Hackathon 2020\n",
    "# Microphysics Machine Learning Challenge Problem\n",
    "\n",
    "Andrew Gettelman, Jack Chen, David John Gagne\n",
    "\n",
    "## Introduction\n",
    "Cloud processes are perhaps the most critical and uncertain processes for weather and climate prediction. The complex nature of sub grid scale clouds makes traceable simulation of clouds across scales difficult (or impossible). There exist many observations and detailed simulations of clouds that are used to develop and evaluate larger scale models. Many times these models and measurements are used to develop empirical relationships for large scale models to be computationally efficient. Machine learning provides another potential tool to improve our empirical parameterizations of clouds. Here we present a comprehensive investigation of replacing the warm rain formation process in an earth system model with emulators that use detailed treatments from small scale and idealized models to represent key cloud microphysical processes. \n",
    "\n",
    "The warm rain formation process is critical for weather and climate prediction. When rain forms governs the location, intensity and duration of rainfall events, critical for weather and the hydrologic cycle. Rain formation also affects cloud lifetime and the radiative properties of low clouds, making it critical for predicting climate (twomey1977,albrecht1989) The specific process of rain formation is altered by the microphysical properties of clouds, making rain formation susceptible to the size distribution of cloud drops, and ultimately to the distribution of aerosol particles that act as Cloud Condensation Nuclei. \n",
    "\n",
    "Ice of course will complicate the precipitation process. Supercooled liquid drops can exist, and these will either precipitation in a similar manner to warm precipitation (with no ice involved) and subsequently may freeze once they are rain drops. Or cloud droplets may freeze and form ice crystals, which precipitate and collect liquid, freezing or riming as they fall. We will not concern ourselves in this work with processes involving (or potentially involving) ice. This of course is a critical issue for weather (forbes2014)and climate (gettelman2019b,bodas-salcedo2019)prediction. \n",
    "\n",
    "The representation of rain formation in clouds involves the interaction of a population of hydrometeors. For warm clouds, the process is one of collision and coalescence, usually defined with a detailed process of stochastic collection (pruppacher1997). The stochastic collection process describes how each size particle interacts with other sizes. Usually there is a distribution of small cloud drops with an extension or separate distribution of rain drops whose interactions are evaluated. \n",
    "\n",
    "The stochastic collection process is computationally expensive to treat directly in large scale global models for weather and climate prediction. It requires the pre-computation of a collection kernel for how different sizes of hydrometeors will interact due to differential fall speeds, and it requires tracking populations discretized by bins. This tracking and advection of the order of 60 different bins for liquid and ice combined makes it computationally expensive. So traditionally, large scale models with bulk microphysics treat the stochastic collection process of warm rain formation in a heavily parameterized fashion (khairoutdinov2000,seifert200) For conceptual simplicity, the process is often broken up into two processes. Autoconversion is the transition of cloud drops into rain as part of a cloud droplet distribution grows to large sizes. Methods for determining autoconversion and accretion are varied. Because they are the major loss mechanism for cloud water different descriptions of the processes result in very different model evolution and climates (michibata2015).\n",
    "\n",
    "Because many methods for autoconversion and accretion are just empirical fits to data or other models, they are readily applicable to replacement with more sophisticated tools. Neural Networks are multivariate emulators that allow many more degrees of freedom than traditional polynomial methods for example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "The data summary should contain the following pieces of information:\n",
    "* Data generation procedure (satellite, model, etc.) \n",
    "* Link to website containing more information about dataset\n",
    "* Time span of the dataset\n",
    "* Geographic coverage of the dataset\n",
    "* Parameter space coverage (if synthetic)\n",
    "\n",
    "The Community Atmosphere Model version 6 (CAM6) is the atmospheric component of the Community Earth System Model version 2 (danabasoglu2020). CAM6 features a two-moment stratiform cloud microphysics scheme [hereafter MG2](gettelman2015b,gettelman2015a) with prognostic liquid, ice, rain and snow hydrometeor classes. MG2 permits ice supersaturation. CAM6 includes a physically based ice mixed phase dust ice nucleation scheme (hoose2010) with modifications for a distribution of contact angles (wang2014), and accounts for preexisting ice in the cirrus ice nucleation of (liu2005) as described by (shi2015).\n",
    "\n",
    "MG2 is coupled to a unified moist turbulence scheme, Cloud Layers Unified by Binormals (CLUBB), developed by (golaz2002) and (larson2002) and implemented in CAM by (bogenschutz2013). CLUBB handles stratiform clouds, boundary layer moist turbulence and shallow convective motions. CAM6 also has an ensemble plume mass flux deep convection scheme described by (zhang1995) and (neale2008), which has very simple microphysics. The radiation scheme is The Rapid Radiative Transfer Model for General Circulation Models (RRTMG) (iacono2000).\n",
    "\n",
    "Within the MG2 parameterization, the warm rain formation process is represented by equations for autoconversion and accretion from (khairoutdinov2000), hereafter KK2000. KK2000 uses empirical fits to a large eddy simulation with bin-resolved microphysics to define:\n",
    "\\begin{equation}\n",
    "    \\left(\\frac{\\partial q_r}{\\partial t} \\right)_{AUTO} = 13.5 q_c^{2.47} N_c^{-1.1}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\left(\\frac{\\partial q_r}{\\partial t} \\right)_{ACCRE} = 67 (q_c q_r)^{1.15}\n",
    "\\end{equation}\n",
    "Where $q_c$ and $q_r$ are mass mixing ratios for condensate and rain, and $N_c$ is the number concentration of condensate. For CAM6 the autconversion rate exponent and prefactor has been adjusted from the original (khairoutdinov2000) scheme to better match observations (gettelman2019b).\n",
    "\n",
    "\\subsection{Stochastic Collection}\n",
    "\\label{sec:tau}\n",
    "\n",
    "We replace the KK2000 process rate equations with an estimate of the stochastic collection process from the Tel Aviv University (TAU) model. The TAU model uses a \"bin\" or \"sectional\" approach, where the drop size distribution is resolved into 35 size bins. It differs from most other microphysical codes in that it solves for two  moments of the drop size distribution in each of the bins. This allows for a more accurate transfer of mass between bins and alleviates anomalous drop growth. The original components were developed by Tzivion et al. (1987), (1989), Feingold et al. (1988) with later applications and development documented in Reisin et al. (1996), Stevens et al. (1996), Feingold et al. (1999), Tzivion et al. (1999), Yin et al (2000) and Harrington et al. (2000). \n",
    "\n",
    "%refs and description from here: https://www.esrl.noaa.gov/csl/staff/graham.feingold/code/readme.html\n",
    "\n",
    "First we convert the size distributions for liquid and rain into number concentrations in individual size bins. Liquid and rain are put in the same continuous distribution of 32 [Check with Jack] size bins for the TAU code. Then we use this as input to the TAU code, running the stochastic collection kernel. The result is a revised set of 32 bins with number concentration in each bin. We the find a minimum in the distribution if present: this is always found in the case where there is rain and condensate present at the end of the calculation. The minimum is typically between 40 and 100 microns (diameter). This minimium is used to divide the bins into liquid and rain. The total number and mass in each is defined, and tendencies calculated as the final mass and number minus the initial mass and number divided by the timestep. A limiter is applied to ensure that the mass and number are non-zero, and tendencies limited to ensure this. This estimated stochastic collection tendency is then applied instead of the accretion and autoconversion tendencies.\n",
    "\n",
    "The code does run the accretion and autoconversion from MG2 on the same state, and we can save this off as a diagnostic, so we can directly compare the original MG2 tendency (autoconversion + accretion) with the stochastic collection tendency from the TAU code. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Input Variables\n",
    "| Variable Name | Units | Description | Relevance |\n",
    "| ------------- | :----:|:----------- | :--------:|\n",
    "| Temperature   | K     | How hot or cold it is | ¯\\\\_(ツ)_/¯ |\n",
    "| Humidity      | %     | Moist       |\n",
    "\n",
    "### Output Variables\n",
    "| Variable Name | Units | Description |\n",
    "| ------------- | :----:|:----------- |\n",
    "| Temperature   | K     | How hot or cold it is |\n",
    "| Humidity      | %     | Moist       |\n",
    "\n",
    "### Metadata Variables\n",
    "| Variable Name | Units | Description |\n",
    "| ------------- | :----:|:----------- |\n",
    "| Date   | YYYY-MM-DD    | The Date   |\n",
    "| Lat      | degrees     | Latitude   |\n",
    "| Lon      | degrees     | Longitude  |\n",
    "\n",
    "\n",
    "### Training Set\n",
    "Description of training set time/space/parameter coverage and size\n",
    "\n",
    "### Validation Set\n",
    "Description of validation set time/space/parameter coverage and size\n",
    "\n",
    "### Test Set\n",
    "Description of test set time/space/parameter coverage and size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import yaml\n",
    "from os.path import join, exists\n",
    "import os\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, mean_absolute_error\n",
    "\n",
    "sys.path.append('../../mlmicrophysics/mlmicrophysics/')\n",
    "\n",
    "from mlmicrophysics.metrics import heidke_skill_score, peirce_skill_score, hellinger_distance, root_mean_squared_error, r2_corr\n",
    "from mlmicrophysics.models import DenseNeuralNetwork\n",
    "from mlmicrophysics.data import subset_data_files_by_date, assemble_data_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {\"MinMaxScaler\": MinMaxScaler,\n",
    "           \"MaxAbsScaler\": MaxAbsScaler,\n",
    "           \"StandardScaler\": StandardScaler,\n",
    "           \"RobustScaler\": RobustScaler}\n",
    "\n",
    "class_metrics = {\"accuracy\": accuracy_score,\n",
    "                 \"heidke\": heidke_skill_score,\n",
    "                 \"peirce\": peirce_skill_score}\n",
    "\n",
    "reg_metrics = {\"rmse\": root_mean_squared_error,\n",
    "               \"mae\": mean_absolute_error,\n",
    "               \"r2\": r2_corr,\n",
    "               \"hellinger\": hellinger_distance}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Machine Learning Model\n",
    "Description of baseline ML approach should include:\n",
    "* Choice of ML software\n",
    "* Type of ML model\n",
    "* Hyperparameter choices and justification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = \"ncar-aiml-data-commons/microphysics\"\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "fs.ls(\"s3://ncar-aiml-data-commons/microphysics\")\n",
    "# fobj = fs.open(filename)\n",
    "# ds = pd.read_parquet(fobj).set_index('Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"ncar-aiml-data-commons/microphysics\"\n",
    "out_path = \"./micro_models/base/\"\n",
    "\n",
    "subset_data = {\"train_date_start\" : 0,\n",
    "               \"train_date_end\" : 9000,\n",
    "               \"test_date_start\" : 9100,\n",
    "               \"test_date_end\" : 17500}\n",
    "\n",
    "random_seed = 328942\n",
    "subsample = 0.1\n",
    "classifier_metrics = [\"acc\", \"pss\", \"hss\"]\n",
    "regressor_metrics = [\"mse\", \"mae\", \"r2\", \"hellinger\"]\n",
    "input_cols = [\"QC_TAU_in\", \"NC_TAU_in\", \"QR_TAU_in\", \"NR_TAU_in\", \"RHO_CLUBB_lev\"]\n",
    "output_cols = [\"qrtend_TAU\", \"nctend_TAU\", \"nrtend_TAU\"]\n",
    "input_scaler = scalers[\"StandardScaler\"]()\n",
    "\n",
    "input_transforms = {\"QC_TAU_in\" : \"log10_transform\",\n",
    "                    \"NC_TAU_in\" : \"log10_transform\",\n",
    "                    \"QR_TAU_in\" : \"log10_transform\",\n",
    "                    \"NR_TAU_in\" : \"log10_transform\"}\n",
    "\n",
    "output_transforms = {\"qrtend_TAU\" : {0: [\"<=\", 1e-18, \"zero_transform\", \"None\"],\n",
    "                                   1: [\">\", 1e-18, \"log10_transform\", \"StandardScaler\"]},\n",
    "                     \"nctend_TAU\" : {0: [\">=\", -1e-18, \"zero_transform\", \"None\"],\n",
    "                                   1: [\"<\", -1e-18, \"neg_log10_transform\", \"StandardScaler\"]},\n",
    "                     \"nrtend_TAU\" : {-1: [\"<\", 0, \"neg_log10_transform\", \"StandardScaler\"],\n",
    "                                   0: [\"==\", 0, \"zero_transform\", \"None\"],\n",
    "                                   1: [\">\", 0, \"log10_transform\", \"StandardScaler\"]}}\n",
    "\n",
    "classifier_networks = {\"hidden_layers\" : 6,\n",
    "                       \"hidden_neurons\" : 30,\n",
    "                       \"loss\" : \"categorical_crossentropy\",\n",
    "                       \"output_activation\" : \"softmax\",\n",
    "                       \"activation\" : \"relu\",\n",
    "                       \"epochs\" : 15,\n",
    "                       \"batch_size\" : 1024,\n",
    "                       \"verbose\" : 1,\n",
    "                       \"lr\" : 0.0001,\n",
    "                       \"l2_weight\" : 1.0e-8,\n",
    "                       \"classifier\" : 1}\n",
    "\n",
    "regressor_networks = {\"hidden_layers\" : 6,\n",
    "                      \"hidden_neurons\" : 30,\n",
    "                      \"loss\" : \"mse\",\n",
    "                      \"output_activation\" : \"linear\",\n",
    "                      \"activation\" : \"relu\",\n",
    "                      \"epochs\" : 15,\n",
    "                      \"batch_size\" : 1024,\n",
    "                      \"verbose\" : 1,\n",
    "                      \"lr\" : 0.0001,\n",
    "                      \"l2_weight\" : 1.0e-8,\n",
    "                      \"classifier\" : 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "if not exists(out_path):\n",
    "    os.makedirs(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset and load train, validation, and test data\n",
    "\n",
    "train_files, val_files, test_files = subset_data_files_by_date(data_path, **subset_data)\n",
    "print(\"Loading training data\")\n",
    "scaled_input_train, \\\n",
    "labels_train, \\\n",
    "transformed_out_train, \\\n",
    "scaled_out_train, \\\n",
    "output_scalers, \\\n",
    "meta_train = assemble_data_files(train_files, input_cols, output_cols, input_transforms,\n",
    "                                     output_transforms, input_scaler, subsample=subsample)\n",
    "\n",
    "print(\"Loading testing data\")\n",
    "scaled_input_test, \\\n",
    "labels_test, \\\n",
    "transformed_out_test, \\\n",
    "scaled_out_test, \\\n",
    "output_scalers_test, \\\n",
    "meta_test = assemble_data_files(test_files, input_cols, output_cols, input_transforms,\n",
    "                                          output_transforms, input_scaler, output_scalers=output_scalers,\n",
    "                                          train=False, subsample=subsample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale input and output data\n",
    "\n",
    "input_scaler_df = pd.DataFrame({\"mean\": input_scaler.mean_, \"scale\": input_scaler.scale_},\n",
    "                               index=input_cols)\n",
    "print(transformed_out_test.columns)\n",
    "print(transformed_out_test.index)\n",
    "meta_test.to_csv(join(out_path, \"meta_test.csv\"), index_label=\"index\")\n",
    "input_scaler_df.to_csv(join(out_path, \"input_scale_values.csv\"), index_label=\"input\")\n",
    "out_scales_list = []\n",
    "for var in output_scalers.keys():\n",
    "    for out_class in output_scalers[var].keys():\n",
    "        print(var, out_class)\n",
    "        if output_scalers[var][out_class] is not None:\n",
    "            out_scales_list.append(pd.DataFrame({\"mean\": output_scalers[var][out_class].mean_,\n",
    "                                                 \"scale\": output_scalers[var][out_class].scale_},\n",
    "                                                index=[var + \"_\" + str(out_class)]))\n",
    "out_scales_df = pd.concat(out_scales_list)\n",
    "print(out_scales_df)\n",
    "out_scales_df.to_csv(join(out_path, \"output_scale_values.csv\"),\n",
    "                     index_label=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and fit the model\n",
    "\n",
    "classifiers = dict()\n",
    "regressors = dict()\n",
    "reg_index = []\n",
    "for output_col in output_cols:\n",
    "    for label in list(output_transforms[output_col].keys()):\n",
    "        if label != 0:\n",
    "            reg_index.append(output_col + f\"_{label:d}\")\n",
    "test_prediction_values = np.zeros((scaled_out_test.shape[0], len(reg_index)))\n",
    "test_prediction_labels = np.zeros(scaled_out_test.shape)\n",
    "classifier_scores = pd.DataFrame(0, index=output_cols, columns=[\"accuracy\", \"heidke\", \"peirce\"])\n",
    "confusion_matrices = dict()\n",
    "reg_cols = [\"rmse\", \"mae\", \"r2\", \"hellinger\"]\n",
    "reg_scores = pd.DataFrame(0, index=reg_index, columns=reg_cols)\n",
    "l = 0\n",
    "\n",
    "for o, output_col in enumerate(output_cols):\n",
    "    print(\"Train Classifer \", output_col)\n",
    "    classifiers[output_col] = DenseNeuralNetwork(**classifier_networks)\n",
    "    classifiers[output_col].fit(scaled_input_train, labels_train[output_col])\n",
    "    classifiers[output_col].save_fortran_model(join(out_path,\n",
    "                                                    \"dnn_{0}_class_fortran.nc\".format(output_col[0:2])))\n",
    "    classifiers[output_col].model.save(join(out_path,\"dnn_{0}_class.h5\".format(output_col[0:2])))\n",
    "    regressors[output_col] = dict()\n",
    "    print(\"Evaluate Classifier\", output_col)\n",
    "    test_prediction_labels[:, o] = classifiers[output_col].predict(scaled_input_test)\n",
    "    confusion_matrices[output_col] = confusion_matrix(labels_test[output_col],\n",
    "                                                      test_prediction_labels  [:, o])\n",
    "    for class_score in classifier_scores.columns:\n",
    "        classifier_scores.loc[output_col, class_score] = class_metrics[class_score](labels_test[output_col],\n",
    "                                                                                    test_prediction_labels[:, o])\n",
    "    print(classifier_scores.loc[output_col])\n",
    "    for label in list(output_transforms[output_col].keys()):\n",
    "        if label != 0:\n",
    "            print(\"Train Regressor \", output_col, label)\n",
    "            regressors[output_col][label] = DenseNeuralNetwork(**regressor_networks)\n",
    "            regressors[output_col][label].fit(scaled_input_train.loc[labels_train[output_col] == label],\n",
    "                                              scaled_out_train.loc[labels_train[output_col] == label, output_col])\n",
    "\n",
    "            if label > 0:\n",
    "                out_label = \"pos\"\n",
    "            else:\n",
    "                out_label = \"neg\"\n",
    "            regressors[output_col][label].save_fortran_model(join(out_path,\n",
    "                                                                  \"dnn_{0}_{1}_fortran.nc\".format(output_col[0:2],\n",
    "                                                                                                  out_label)))\n",
    "            regressors[output_col][label].model.save(join(out_path,\n",
    "                                                          \"dnn_{0}_{1}.h5\".format(output_col[0:2], out_label)))\n",
    "            print(\"Test Regressor\", output_col, label)\n",
    "            test_prediction_values[:, l] = output_scalers[output_col][label].inverse_transform(regressors[output_col][label].predict(scaled_input_test))\n",
    "            reg_label = output_col + f\"_{label:d}\"\n",
    "            for reg_col in reg_cols:\n",
    "                reg_scores.loc[reg_label,\n",
    "                               reg_col] = reg_metrics[reg_col](transformed_out_test.loc[labels_test[output_col] == label,\n",
    "                                                                                        output_col],\n",
    "                                                                test_prediction_values[labels_test[output_col] == label, l])\n",
    "            print(reg_scores.loc[reg_label])\n",
    "            l += 1\n",
    "print(\"Saving data\")\n",
    "classifier_scores.to_csv(join(out_path, \"dnn_classifier_scores.csv\"), index_label=\"Output\")\n",
    "reg_scores.to_csv(join(out_path, \"dnn_regressor_scores.csv\"), index_label=\"Output\")\n",
    "test_pred_values_df = pd.DataFrame(test_prediction_values, columns=reg_index)\n",
    "test_pred_labels_df = pd.DataFrame(test_prediction_labels, columns=output_cols)\n",
    "test_pred_values_df.to_csv(join(out_path, \"test_prediction_values.csv\"), index_label=\"index\")\n",
    "test_pred_labels_df.to_csv(join(out_path, \"test_prediction_labels.csv\"), index_label=\"index\")\n",
    "labels_test.to_csv(join(out_path, \"test_cam_labels.csv\"), index_label=\"index\")\n",
    "transformed_out_test.to_csv(join(out_path, \"test_cam_values.csv\"), index_label=\"index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "Description of the different metrics used to assess performance on the challenge:\n",
    "* Correctness Metric: how close are the predictions to the truth (e.g., RMSE or AUC) \n",
    "* Training time\n",
    "* Inference time\n",
    "* Model complexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "\n",
    "Albrecht, B. A.  (1989).  Aerosols, cloud microphysics and fractional cloudiness.Sci-449ence,245, 1227–1230.\n",
    "\n",
    "Bodas-Salcedo, A., Mulcahy, J. P., Andrews, T., Williams, K. D., Ringer, M. A.,455Field, P. R., & Elsaesser, G. S.(2019).Strong Dependence of Atmospheric456Feedbacks on Mixed-Phase Microphysics and Aerosol-Cloud Interactions in457HadGEM3.Journal of Advances in Modeling Earth Systems,11(6), 1735–1758.458doi:  10.1029/2019MS001688\n",
    "\n",
    "Bogenschutz, P. A., Gettelman, A., Morrison, H., Larson, V. E., Craig, C., & Scha-460nen, D. P.(2013).Higher-order turbulence closure and its impact on Climate461Simulation in the Community Atmosphere Model.Journal of Climate,26(23),4629655–9676.  doi:  10.1175/JCLI-D-13-00075.1\n",
    "\n",
    "Danabasoglu, G., Lamarque, J.-F., Bacmeister, J., Bailey, D. A., DuVivier, A. K.,471Edwards, J., . . .  Strand, W. G.(2020).The Community Earth System Model472Version 2 (CESM2).Journal of Advances in Modeling Earth Systems,12(2),473e2019MS001916.  doi:  10.1029/2019MS001916\n",
    "\n",
    "Forbes, R. M., & Ahlgrimm, M.(2014, September).On the Representation of475High-Latitude Boundary Layer Mixed-Phase Cloud in the ECMWF Global Model.476Monthly Weather Review,142(9), 3425–3445.  doi:  10.1175/MWR-D-13-00325.1\n",
    "\n",
    "Gettelman, A.(2015, November).Putting the clouds back in aerosol–cloud inter-478actions.Atmos. Chem. Phys.,15(21), 12397–12411.doi:  10.5194/acp-15-12397479-2015480\n",
    "\n",
    "Gettelman, A., Bardeen, C. G., McCluskey, C. S., & Jarvinen, E.    (2020).    Simulat-481ing Observations of Southern Ocean Clouds and Implications for Climate.J. Adv.482Model. Earth Syst..  doi:  10.1029/2020JD032619483\n",
    "\n",
    "Gettelman, A., Hannay, C., Bacmeister, J. T., Neale, R. B., Pendergrass, A. G.,484Danabasoglu, G., . . .  Mills, M. J.(2019).High Climate Sensitivity in the Com-485munity Earth System Model Version 2 (CESM2).Geophysical Research Letters,48646(14), 8329–8337.  doi:  10.1029/2019GL083978487\n",
    "\n",
    "Gettelman, A., & Morrison, H.   (2015).   Advanced Two-Moment Bulk Microphysics488for Global Models. Part I: Off-Line Tests and Comparison with Other Schemes.J.489Climate,28(3), 1268–1287.  doi:  10.1175/JCLI-D-14-00102.1490\n",
    "\n",
    "Gettelman, A., Morrison, H., Santos, S., Bogenschutz, P., & Caldwell, P. M.   (2015).491Advanced Two-Moment Bulk Microphysics for Global Models. Part II: Global492Model Solutions and Aerosol–Cloud Interactions.J. Climate,28(3), 1288–1307.493doi:  10.1175/JCLI-D-14-00103.1494\n",
    "\n",
    "Gettelman, A., & Sherwood, S. C.  (2016, October).  Processes Responsible for Cloud495Feedback.Curr Clim Change Rep, 1–11.  doi:  10.1007/s40641-016-0052-8\n",
    "\n",
    "Golaz, J.-C., Larson, V. E., & Cotton, W. R.(2002).A PDF-Based Model for497Boundary Layer Clouds. Part II: Model Results.J. Atmos. Sci.,59, 3552–3571.\n",
    "\n",
    "Hoose, C., Kristj ́ansson, J. E., Chen, J.-P., & Hazra, A.  (2010, March).  A Classical-499Theory-Based Parameterization of Heterogeneous Ice Nucleation by Mineral Dust,500Soot, and Biological Particles in a Global Climate Model.J. Atmos. Sci.,67(8),5012483–2503.  doi:  10.1175/2010JAS3425.1\n",
    "\n",
    "Iacono, M. J., Mlawer, E. J., Clough, S. A., & Morcrette, J.-J.  (2000).  Impact of an503improved longwave radiation model, RRTM, on the energy budget and thermody-504namic properties of the NCAR community climate model, CCM3.jgr,105(D11),50514,873–14,890.\n",
    "\n",
    "Khairoutdinov, M. F., & Kogan, Y.  (2000).  A new cloud physics parameterization in507a large-eddy simulation model of marine stratocumulus.Monthly Weather Review,508128, 229–243.\n",
    "\n",
    "Larson, V. E., Golaz, J.-C., & Cotton, W. R.(2002, December).Small-Scale and510Mesoscale Variability in Cloudy Boundary Layers:  Joint Probability Density Func-511tions.J. Atmos. Sci.,59(24), 3519–3539.   doi:  10.1175/1520-0469(2002)059〈3519:512SSAMVI〉2.0.CO;2\n",
    "\n",
    "Liu, X., & Penner, J. E.  (2005).  Ice Nucleation Parameterization for Global Models.514Meteor. Z.,14(499-514).\n",
    "\n",
    "Michibata, T., & Takemura, T.(2015, September).Evaluation of autoconversion520schemes in a single model framework with satellite observations.J. Geophys. Res.521Atmos.,120(18), 2015JD023818.  doi:  10.1002/2015JD023818\n",
    "\n",
    "Neale, R. B., Richter, J. H., & Jochum, M.(2008).The Impact of Convection on523ENSO: From a Delayed Oscillator to a Series of Events.J. Climate,21, 5904-+.doi:  10.1175/2008JCLI2244.1\n",
    "\n",
    "Pruppacher, H. R., & Klett, J. D.   (1997).Microphysics of Clouds and Precipitation526(Second ed.).  Kluwer Academic.\n",
    "\n",
    "Seifert, A., & Beheng, K. D.  (2001).  A double-moment parameterization for simulat-531ing autoconversion, accretion and selfcollection.Atmos. Res.,59-60, 265–281.\n",
    "\n",
    "Shi, X., Liu, X., & Zhang, K.  (2015, February).  Effects of pre-existing ice crystals on536cirrus clouds and comparison between different ice nucleation parameterizations537with the Community Atmosphere Model (CAM5).Atmospheric Chemistry and538Physics,15(3), 1503–1520.  doi:  10.5194/acp-15-1503-2015\n",
    "\n",
    "Twomey, S.  (1977).  The influence of pollution on the shortwave albedo of clouds.J.553Atmos. Sci.,34(7), 1149–1152.\n",
    "\n",
    "Wang, Y., Liu, X., Hoose, C., & Wang, B.(2014, October).Different contact555angle distributions for heterogeneous ice nucleation in the Community Atmo-556spheric Model version 5.Atmos. Chem. Phys.,14(19), 10411–10430.doi:55710.5194/acp-14-10411-2014\n",
    "\n",
    "Zhang, G. J., & McFarlane, N. A.    (1995).    Sensitivity of climate simulations to the559parameterization of cumulus convection in the Canadian Climate Center general560circulation model.Atmos. Ocean,33, 407–446."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
